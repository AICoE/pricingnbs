{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement apsw==3.33.0.post1\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for apsw==3.33.0.post1\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from profilehooks import profile\n",
    "    \n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get number of threads and choose range of number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROC = 4 #CHANGE THIS\n",
    "\n",
    "N_CLUSTER_LOW = 5 #CAN CHANGE THIS\n",
    "N_CLUSTER_HIGH = 15 #CAN CHANGE THIS\n",
    "\n",
    "N_INIT = 1 #DON'T CHANGE THIS\n",
    "N_INIT_TOTAL = 10 #DON'T CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workload\n",
    "\n",
    "This workload runs K-means clustering on the MNIST dataset for a range of n_cluster values. The idea is to see how well a simple clustering on raw pixel values segments the data since we have labels. There are no embeddings of the data using a neural network and no feature transformations apart from scaling each feature to mean = 0 and standard deviation = 1.\n",
    "\n",
    "Note that K-means is run on the full dataset for a range of n_cluster values. In addition, for each n_cluster value, by default, scikit-learn's implementation runs the algorithm n_init = 10 times with different random initializations of the cluster locations. Each of these runs is distributed across all available threads by scikit-learn. In addition, we run the range of cluster_values in parallel using multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST('./', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/venv_general/lib64/python3.8/site-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "#flatten each image\n",
    "data = data.train_data.numpy().reshape(data.train_data.shape[0], -1).astype(np.float32)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, data,                \n",
    "                 n_clusters_low=2, \n",
    "                 n_clusters_high=50, \n",
    "                 n_clusters_stepsize=5, \n",
    "                 n_processes=None):\n",
    "\n",
    "        self.data = data.copy()\n",
    "\n",
    "        self.data_normalized = None  # zscore -> pca -> zscore\n",
    "\n",
    "        #clustering scan\n",
    "        self.n_clusters_low = n_clusters_low\n",
    "        self.n_clusters_high = n_clusters_high\n",
    "        self.n_clusters_stepsize = n_clusters_stepsize\n",
    "        self.n_processes = n_processes\n",
    "\n",
    "        self.models_dict = None\n",
    "        self.inertia_dict = None\n",
    "\n",
    "        #clustering optimal\n",
    "        self.n_clusters_optimal = None\n",
    "        self.kmeans_optimal = None\n",
    "\n",
    "    @profile(immediate=True)\n",
    "    def train(self):\n",
    "        self.preprocess_normalize()\n",
    "\n",
    "        if self.n_processes is None:\n",
    "            self.find_nclusters()\n",
    "        else:\n",
    "            self.find_nclusters_parallel()\n",
    "\n",
    "        #self.find_elbow()  # hard-coded currently\n",
    "\n",
    "\n",
    "    def preprocess_normalize(self):        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.data_normalized = self.scaler.fit_transform(self.data)\n",
    "\n",
    "    def find_nclusters(self):\n",
    "        raise ValueError('You should be passing a value for n_processes')\n",
    "        \n",
    "        print('Calling find_nclusters...')\n",
    "        if self.data_normalized is None:\n",
    "            raise AttributeError(\"Please call preprocess_normalize to prepare data for clustering.\")\n",
    "\n",
    "        self.inertia_dict, self.models_dict = {}, {}\n",
    "        for ncl in range(self.n_clusters_low, self.n_clusters_high, self.n_clusters_stepsize):\n",
    "            kmeans = KMeans(n_clusters=ncl, n_init=N_INIT, n_jobs=N_PROC)\n",
    "            kmeans.fit(self.data_normalized)\n",
    "\n",
    "            self.inertia_dict[ncl] = kmeans.inertia_\n",
    "            self.models_dict[ncl] = kmeans\n",
    "\n",
    "    def find_nclusters_parallel(self):\n",
    "        print('Calling find_nclusters_parallel...')\n",
    "        manager = mp.Manager()\n",
    "        models_shared_dict = manager.dict()\n",
    "        inertia_shared_dict = manager.dict()\n",
    "\n",
    "        proc_list = []\n",
    "        counter = 0\n",
    "        def run(data, ncl, init_id):\n",
    "            kmeans = KMeans(n_clusters=ncl, n_init=N_INIT)\n",
    "            kmeans.fit(data)\n",
    "\n",
    "            inertia_shared_dict[(ncl, init_id)] = kmeans.inertia_\n",
    "            models_shared_dict[(ncl, init_id)] = kmeans\n",
    "\n",
    "        for ncl in range(self.n_clusters_low, self.n_clusters_high+1, self.n_clusters_stepsize):\n",
    "            for init_id in range(N_INIT_TOTAL):\n",
    "                proc = mp.Process(target=run, args=(self.data_normalized, ncl, init_id))\n",
    "                proc.start()\n",
    "                proc_list.append(proc)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % self.n_processes == 0:\n",
    "                    [p.join() for p in  proc_list]\n",
    "                    proc_list = []\n",
    "\n",
    "                [p.join() for p in proc_list]\n",
    "\n",
    "        self.inertia_dict = dict(inertia_shared_dict)\n",
    "        self.models_dict = dict(models_shared_dict)\n",
    "    \n",
    "    def find_best_ninit(self):\n",
    "        inertia_dict, models_dict = {}, {}\n",
    "\n",
    "        for ncl in range(N_CLUSTER_LOW, N_CLUSTER_HIGH+1):\n",
    "\n",
    "            min_inertia_val = self.inertia_dict[(ncl, 0)]\n",
    "            min_iter_id = 0\n",
    "\n",
    "            for iter_id in range(1, N_INIT_TOTAL):\n",
    "                current_inertia_val = self.inertia_dict[(ncl, iter_id)]\n",
    "\n",
    "                if current_inertia_val < min_inertia_val:\n",
    "                    min_inertia_val = current_inertia_val\n",
    "                    min_iter_id = iter_id\n",
    "\n",
    "            #store best result\n",
    "            models_dict[ncl] = self.models_dict[(ncl, min_iter_id)]\n",
    "            inertia_dict[ncl] = self.inertia_dict[(ncl, min_iter_id)]                \n",
    "\n",
    "            assert(models_dict[ncl].inertia_==inertia_dict[ncl])\n",
    "\n",
    "        self.models_dict_fine = copy.deepcopy(self.models_dict)\n",
    "        self.inertia_dict_fine = copy.deepcopy(self.inertia_dict)\n",
    "        \n",
    "        self.models_dict = models_dict\n",
    "        self.inertia_dict = inertia_dict        \n",
    "    \n",
    "    def find_elbow(self):\n",
    "        if self.inertia_dict is None:\n",
    "            raise AttributeError(\"Please run find_nclusters to populated inertia_dict.\")\n",
    "        \n",
    "        keys = np.sort(list(self.inertia_dict.keys()))\n",
    "        values = np.array([self.inertia_dict[k] for k in keys])\n",
    "\n",
    "        N = len(keys)\n",
    "\n",
    "        thresholds = np.arange(2, N-1) #indices to slice at\n",
    "\n",
    "        threshold_cuts, score_means = [], []\n",
    "\n",
    "        for t in thresholds:\n",
    "            model1, model2 = LinearRegression(), LinearRegression()\n",
    "\n",
    "            domain1 = np.arange(t)\n",
    "            domain2 = np.arange(t, N)\n",
    "            \n",
    "            model1.fit(keys[domain1].reshape(-1,1), values[domain1])\n",
    "            model2.fit(keys[domain2].reshape(-1,1), values[domain2])\n",
    "\n",
    "            score1 = model1.score(keys[domain1].reshape(-1,1), values[domain1])\n",
    "            score2 = model2.score(keys[domain2].reshape(-1,1), values[domain2])\n",
    "\n",
    "            score = (score1+score2)/2.0\n",
    "            \n",
    "            threshold_cuts.append(keys[t])\n",
    "            score_means.append(score)\n",
    "\n",
    "        self.n_clusters_optimal = threshold_cuts[np.argmax(score_means)]\n",
    "\n",
    "        return threshold_cuts, score_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling find_nclusters_parallel...\n"
     ]
    }
   ],
   "source": [
    "cl = Cluster(data, n_clusters_low=N_CLUSTER_LOW, n_clusters_high=N_CLUSTER_HIGH, n_clusters_stepsize=1, n_processes=N_PROC)\n",
    "cl.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.find_best_ninit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.inertia_dict)\n",
    "print(cl.inertia_dict_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in the table below is a single cluster. The rows show the number of images in a particular cluster that belong to each digit (rows).\n",
    "\n",
    "E.g.:\n",
    "\n",
    "Cluster 9 contains both \"7\" and \"9\" which are similar in shape\n",
    "\n",
    "Cluster 2 is dominated by \"0\"s\n",
    "\n",
    "Cluster 1 contains \"4\", \"7\" and \"9\" which are also similar in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 5 in cl.models_dict:\n",
    "    pred = cl.models_dict[5].predict(cl.data_normalized)\n",
    "    labels = MNIST('./', download=True).train_labels.numpy()\n",
    "    \n",
    "    comp = pd.DataFrame({'clusterid': pred, 'label': labels})\n",
    "    comp['count'] = 1\n",
    "    table = pd.pivot_table(comp, index='label', columns='clusterid', values='count', aggfunc='sum')\n",
    "    \n",
    "table.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method to find the optimal number of clusters is to look at the so-called elbow plot of the inertia (sum of distances of each point from its cluster's center) vs the number of clusters. As the number of clusters approaches the number of data points, inertia approaches 0 (since each point is its own clusters). Often, there is a num_cluster values at which the plot shows a dramatic change in slope (slope becomes less negative i.e. increases). This doesn't happen here which is expected since even some variation in an image (pixel-wise translation) can result in a big increase in Euclidean distance but none in the label.\n",
    "\n",
    "One way to automatically identify the elbow plot is to split the number of clusters axis at different points and fit both the left and right side of the point with linear models. If there's a sharp elbow, both fits should be very good leading to a local maximum of the average R^2 metric. This is implemented in Cluster.find_elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(cl.inertia_dict.keys()), list(cl.inertia_dict.values()), 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cuts, score_means = cl.find_elbow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threshold_cuts, score_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the best average $R^2$ score at n_clusters = 10 indicating we should pick 10 as the value for num_clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
